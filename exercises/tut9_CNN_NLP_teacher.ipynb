{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Humboldt-WI/adams/blob/master/exercises/tut9_CNN_NLP_teacher.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 9: Convolutional Neural Nets for Text Data\n",
    "In this tutorial, we will first explain what the layers `Conv2D` (rank-3 tensors) and `Conv1D` (rank-2 tensors) do. Then, we will use `Conv1D` to classify Tweets into positive, neutral and negative sentimentsâ€”the Tweets are from the clients of different airlines. \n",
    "\n",
    "For further examples, please visit [demos/cnn](https://github.com/Humboldt-WI/adams/tree/master/demos/cnn)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "import string\n",
    "import re\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ConveNets\n",
    "Convnets are widely used in computer vision applications. The most common is the `Conv2D` which takes as input tensors of shape `(height, width, channels)` plus the batch. Let's see a simple example "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 3, 3, 2), dtype=float32, numpy=\n",
       "array([[[[1., 2.],\n",
       "         [1., 2.],\n",
       "         [1., 2.]],\n",
       "\n",
       "        [[1., 2.],\n",
       "         [1., 2.],\n",
       "         [1., 2.]],\n",
       "\n",
       "        [[1., 2.],\n",
       "         [1., 2.],\n",
       "         [1., 2.]]]], dtype=float32)>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a sample input (batch, height, width, channels)\n",
    "tf.random.set_seed(1234) # for reproducibility\n",
    "ex_input = tf.concat([tf.ones((1,3,3,1)), 2*tf.ones((1,3,3,1))], axis=3 ) # (1,3,3,2)\n",
    "ex_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 2, 2, 1), dtype=float32, numpy=\n",
       "array([[[[-0.27699053],\n",
       "         [-0.27699053]],\n",
       "\n",
       "        [[-0.27699053],\n",
       "         [-0.27699053]]]], dtype=float32)>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply a convnet with 1 filter and a kernel of size 2\n",
    "cnn2D = layers.Conv2D(filters=1,kernel_size=2, input_shape=ex_input.shape[1:])\n",
    "cnn2D(ex_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 2, 2, 1)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's understand the matrix operations\n",
    "kernel = cnn2D.get_weights()[0] # random initialization weights\n",
    "kernel.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.05379575],\n",
       "        [ 0.1154424 ]],\n",
       "\n",
       "       [[-0.09852028],\n",
       "        [ 0.01021165]]], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kernel[:,:,0,:] # weights of first channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.27699053"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(1*kernel[:,:,0,:])+np.sum(2*kernel[:,:,1,:]) # replicate the firts output of the convnet\n",
    "# np.sum(np.multiply(tf.ones(2,2), kernel[:,:,0,0])) + np.sum(np.multiply(2*tf.ones(2,2), kernel[:,:,1,0])) # replicate the firts output of the convnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convnets are not restricted to rank-3 tensor `(height, width, channels)`. Keras also has `Conv3D` and `Conv1D` implemented. Let's look at `Conv1D`, which requires a rank-2 tensor as input, such as sequence data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 3, 2), dtype=float32, numpy=\n",
       "array([[[1., 1.],\n",
       "        [2., 2.],\n",
       "        [3., 3.]]], dtype=float32)>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Input for cnn1D (batch, seq_length, emb_dim)\n",
    "tf.random.set_seed(1234)\n",
    "ex_input = tf.concat([tf.ones((1,1,2)), 2*tf.ones((1,1,2)), 3*tf.ones((1,1,2))], axis = 1) # (1, 3, 2)\n",
    "ex_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 2, 1), dtype=float32, numpy=\n",
       "array([[[-0.8928499],\n",
       "        [-1.4366169]]], dtype=float32)>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply a convnet with 1 filter and a kernel of size 2\n",
    "cnn1D = layers.Conv1D(filters=1,kernel_size=2, input_shape=ex_input.shape[1:])\n",
    "cnn1D(ex_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 2, 1)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kernel = cnn1D.get_weights()[0]\n",
    "kernel.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.8928499\n",
      "-1.4366169\n"
     ]
    }
   ],
   "source": [
    "print(np.sum(1*kernel[0,:,:] + 2*kernel[1,:,:] )) # first row and second row\n",
    "print(np.sum(2*kernel[0,:,:] + 3*kernel[1,:,:] )) # secod row and third row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tweets classification\n",
    "The purpose is to put `Conv1D` into practice. We have Twitter data concerning airline clients and the labels of their tweets (positive, neutral, negative). The idea is to create a classification model for tweets. We'll only care about the positive and negative in the first part. Then, we include the neutral labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "tot_tweets = pd.read_csv(\"../../../demos/cnn/Tweets.csv.zip\")\n",
    "tot_tweets = tot_tweets[['airline_sentiment','text']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Positive and Negative Tweets\n",
    "\n",
    "### Exercise 1: \n",
    "Remove the samples with the label `neutral`, create train and validation sets, and then transform them to NumPy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove neutral labels and transform to numpy\n",
    "tweets = tot_tweets[tot_tweets['airline_sentiment']!='neutral'].copy()\n",
    "tweets['airline_sentiment'] = tweets['airline_sentiment'].map({'positive' : 1, 'negative': 0})\n",
    "X_train, X_val, y_train, y_val = train_test_split(tweets['text'], tweets['airline_sentiment'], test_size = 0.2, random_state = 5)\n",
    "X_train = X_train.to_numpy()\n",
    "X_val = X_val.to_numpy()\n",
    "y_train = y_train.to_numpy()\n",
    "y_val = y_val.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2:\n",
    "Create a function to standardize the text. In particular, replace any character that is not a-z OR A-Z with a space, convert to lowercase, and remove punctuation and double space. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define standarization function \n",
    "def our_standardization(text_data):\n",
    "  remove_non = tf.strings.regex_replace(text_data, '[^a-zA-Z]', ' ') # replace non a-z OR A-Z with \" \"\n",
    "  lowercase = tf.strings.lower(remove_non) # convert to lowercase\n",
    "  pattern_remove_punctuation = '[%s]' % re.escape(string.punctuation) # pattern to remove punctuation\n",
    "  remove_punct = tf.strings.regex_replace(lowercase, pattern_remove_punctuation, '') # apply pattern\n",
    "  remove_double_spaces = tf.strings.regex_replace(remove_punct, '\\s+', ' ') # remove double space\n",
    "  remove_initial_end_spaces  =tf.strings.regex_replace(remove_double_spaces, '^\\s*|\\s*$', '') \n",
    "  return remove_initial_end_spaces\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3:\n",
    "Create a vectorization layer and apply it to the text data. Use 10000 tokens with a maximum length for each tweet of 50. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size=10000\n",
    "seq_length = 50\n",
    "# Create a vectorization layer\n",
    "vectorize_layer = TextVectorization(\n",
    "    standardize = our_standardization,\n",
    "    max_tokens = vocab_size,\n",
    "    output_sequence_length = seq_length\n",
    "    )\n",
    "vectorize_layer.adapt(X_train)\n",
    "\n",
    "## Transform sequences of words to seq of integers and labels to tensor\n",
    "X_train = vectorize_layer(X_train)\n",
    "X_val = vectorize_layer(X_val)\n",
    "y_train = tf.convert_to_tensor(y_train)\n",
    "y_val = tf.convert_to_tensor(y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model `Embedding` + `Conv1D` + `MaxPooling1D` + `Flatten` + `Dense`\n",
    "### Exercise 4:\n",
    "Create a model with one `Embedding` of dimension 16, followed by a `Conv1D` with 32 filters and a kernel size of 8 and relu activation. Then, apply `MaxPooling1D` with a pool size of 2, `Flatten` the output and finally use the `Dense` layer. Can you explain the number of parameters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 50)]              0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 50, 16)            160000    \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 43, 32)            4128      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 21, 32)            0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 672)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 673       \n",
      "=================================================================\n",
      "Total params: 164,801\n",
      "Trainable params: 164,801\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "emb_size = 16\n",
    "num_filters = 32\n",
    "ker_size = 8\n",
    "\n",
    "inputs = tf.keras.Input(shape = (seq_length, ))\n",
    "emb = layers.Embedding(input_dim=vocab_size, output_dim=emb_size)(inputs) \n",
    "x = layers.Conv1D(filters = num_filters, kernel_size = ker_size, activation = 'relu')(emb)\n",
    "x = layers.MaxPooling1D(2)(x)\n",
    "x = layers.Flatten()(x)\n",
    "outputs = layers.Dense(1, activation='sigmoid')(x)\n",
    "model = tf.keras.Model(inputs, outputs)\n",
    "\n",
    "model.compile(optimizer=\"rmsprop\",\n",
    "              loss=\"binary_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5: \n",
    "Train the model using a batch size of 128 for 20 epochs and an `EarlyStopping` callback with patience of 3. Restore the best weights and evaluate the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "73/73 - 1s - loss: 0.4664 - accuracy: 0.8006 - val_loss: 0.3941 - val_accuracy: 0.8268\n",
      "Epoch 2/20\n",
      "73/73 - 0s - loss: 0.2980 - accuracy: 0.8744 - val_loss: 0.2770 - val_accuracy: 0.8826\n",
      "Epoch 3/20\n",
      "73/73 - 0s - loss: 0.1970 - accuracy: 0.9269 - val_loss: 0.2167 - val_accuracy: 0.9173\n",
      "Epoch 4/20\n",
      "73/73 - 0s - loss: 0.1484 - accuracy: 0.9453 - val_loss: 0.1950 - val_accuracy: 0.9233\n",
      "Epoch 5/20\n",
      "73/73 - 0s - loss: 0.1216 - accuracy: 0.9556 - val_loss: 0.2145 - val_accuracy: 0.9207\n",
      "Epoch 6/20\n",
      "73/73 - 0s - loss: 0.1034 - accuracy: 0.9618 - val_loss: 0.1908 - val_accuracy: 0.9268\n",
      "Epoch 7/20\n",
      "73/73 - 0s - loss: 0.0888 - accuracy: 0.9675 - val_loss: 0.2014 - val_accuracy: 0.9199\n",
      "Epoch 8/20\n",
      "73/73 - 0s - loss: 0.0780 - accuracy: 0.9716 - val_loss: 0.2071 - val_accuracy: 0.9225\n",
      "Epoch 9/20\n",
      "73/73 - 1s - loss: 0.0685 - accuracy: 0.9763 - val_loss: 0.2425 - val_accuracy: 0.9199\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fea3a9cd640>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "callbacks = [tf.keras.callbacks.EarlyStopping(monitor = 'val_loss', patience = 3,restore_best_weights=True)]\n",
    "\n",
    "model.fit(\n",
    "    X_train, \n",
    "    y_train, \n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs = 20,\n",
    "    batch_size = 128,\n",
    "    callbacks=callbacks,\n",
    "    verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1908 - accuracy: 0.9268\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9268081188201904"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_val, y_val)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model `Embedding` + `Conv1D` + `GlobalAveragePooling1D` + `Dense`\n",
    "### Exercise 6:\n",
    "Create a new model similar to the previous one but replace the `MaxPooling1D` and `Flatten` layers with `GlobalAveragePooling1D`. Can you explain what we are doing? Next, train the model using the previous settings. Is it better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, 50)]              0         \n",
      "_________________________________________________________________\n",
      "embedding_2 (Embedding)      (None, 50, 16)            160000    \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 43, 32)            4128      \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d (Gl (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 164,161\n",
      "Trainable params: 164,161\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "emb_size = 16\n",
    "num_filters = 32\n",
    "ker_size = 8\n",
    "\n",
    "inputs = tf.keras.Input(shape = (seq_length, ))\n",
    "emb = layers.Embedding(input_dim=vocab_size, output_dim=emb_size)(inputs) \n",
    "x = layers.Conv1D(filters = num_filters, kernel_size = ker_size, activation = 'relu')(emb)\n",
    "x = layers.GlobalAveragePooling1D()(x)\n",
    "outputs = layers.Dense(1, activation='sigmoid')(x)\n",
    "model = tf.keras.Model(inputs, outputs)\n",
    "\n",
    "model.compile(optimizer=\"rmsprop\",\n",
    "              loss=\"binary_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "73/73 - 1s - loss: 0.5483 - accuracy: 0.7869 - val_loss: 0.5024 - val_accuracy: 0.7878\n",
      "Epoch 2/20\n",
      "73/73 - 0s - loss: 0.4441 - accuracy: 0.7971 - val_loss: 0.4142 - val_accuracy: 0.8216\n",
      "Epoch 3/20\n",
      "73/73 - 0s - loss: 0.3603 - accuracy: 0.8414 - val_loss: 0.3619 - val_accuracy: 0.8411\n",
      "Epoch 4/20\n",
      "73/73 - 1s - loss: 0.3078 - accuracy: 0.8683 - val_loss: 0.3337 - val_accuracy: 0.8553\n",
      "Epoch 5/20\n",
      "73/73 - 0s - loss: 0.2640 - accuracy: 0.8947 - val_loss: 0.2923 - val_accuracy: 0.8779\n",
      "Epoch 6/20\n",
      "73/73 - 0s - loss: 0.2256 - accuracy: 0.9143 - val_loss: 0.2636 - val_accuracy: 0.8965\n",
      "Epoch 7/20\n",
      "73/73 - 0s - loss: 0.1948 - accuracy: 0.9285 - val_loss: 0.2366 - val_accuracy: 0.9056\n",
      "Epoch 8/20\n",
      "73/73 - 0s - loss: 0.1730 - accuracy: 0.9399 - val_loss: 0.2284 - val_accuracy: 0.9142\n",
      "Epoch 9/20\n",
      "73/73 - 0s - loss: 0.1555 - accuracy: 0.9471 - val_loss: 0.2286 - val_accuracy: 0.9138\n",
      "Epoch 10/20\n",
      "73/73 - 0s - loss: 0.1417 - accuracy: 0.9513 - val_loss: 0.2128 - val_accuracy: 0.9164\n",
      "Epoch 11/20\n",
      "73/73 - 0s - loss: 0.1303 - accuracy: 0.9542 - val_loss: 0.2135 - val_accuracy: 0.9155\n",
      "Epoch 12/20\n",
      "73/73 - 0s - loss: 0.1211 - accuracy: 0.9571 - val_loss: 0.2528 - val_accuracy: 0.8991\n",
      "Epoch 13/20\n",
      "73/73 - 0s - loss: 0.1129 - accuracy: 0.9612 - val_loss: 0.2076 - val_accuracy: 0.9229\n",
      "Epoch 14/20\n",
      "73/73 - 0s - loss: 0.1065 - accuracy: 0.9637 - val_loss: 0.2100 - val_accuracy: 0.9233\n",
      "Epoch 15/20\n",
      "73/73 - 0s - loss: 0.0991 - accuracy: 0.9651 - val_loss: 0.2130 - val_accuracy: 0.9242\n",
      "Epoch 16/20\n",
      "73/73 - 0s - loss: 0.0932 - accuracy: 0.9682 - val_loss: 0.2608 - val_accuracy: 0.9034\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fea3aa281f0>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "callbacks = [tf.keras.callbacks.EarlyStopping(monitor = 'val_loss', patience = 3,restore_best_weights=True)]\n",
    "\n",
    "model.fit(\n",
    "    X_train, \n",
    "    y_train, \n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs = 20,\n",
    "    batch_size = 128,\n",
    "    callbacks=callbacks,\n",
    "    verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73/73 [==============================] - 0s 1ms/step - loss: 0.2076 - accuracy: 0.9229\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9229103326797485"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_val, y_val)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model `Embedding` + `Conv1D` + `MaxPooling1D`+ `Conv1D` + `MaxPooling1D` + `Flatten` + `Dense`\n",
    "### Exercise 7:\n",
    "Let's try now a deeper network by adding `Conv1D` + `MaxPooling1D` to the first configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         [(None, 50)]              0         \n",
      "_________________________________________________________________\n",
      "embedding_3 (Embedding)      (None, 50, 16)            160000    \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 43, 32)            4128      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 21, 32)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 18, 32)            4128      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 9, 32)             0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 288)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 289       \n",
      "=================================================================\n",
      "Total params: 168,545\n",
      "Trainable params: 168,545\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "emb_size = 16\n",
    "num_filters = 32\n",
    "ker_size = 8\n",
    "\n",
    "inputs = tf.keras.Input(shape = (seq_length, ))\n",
    "emb = layers.Embedding(input_dim=vocab_size, output_dim=emb_size)(inputs) \n",
    "x = layers.Conv1D(filters = num_filters, kernel_size = ker_size, activation = 'relu')(emb)\n",
    "x = layers.MaxPooling1D(2)(x)\n",
    "x = layers.Conv1D(filters = num_filters, kernel_size = int(ker_size/2), activation = 'relu')(x)\n",
    "x = layers.MaxPooling1D(2)(x)\n",
    "x = layers.Flatten()(x)\n",
    "outputs = layers.Dense(1, activation='sigmoid')(x)\n",
    "model = tf.keras.Model(inputs, outputs)\n",
    "\n",
    "model.compile(optimizer=\"rmsprop\",\n",
    "              loss=\"binary_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "73/73 - 2s - loss: 0.4693 - accuracy: 0.7941 - val_loss: 0.4095 - val_accuracy: 0.8172\n",
      "Epoch 2/20\n",
      "73/73 - 1s - loss: 0.2995 - accuracy: 0.8740 - val_loss: 0.2875 - val_accuracy: 0.8688\n",
      "Epoch 3/20\n",
      "73/73 - 1s - loss: 0.1912 - accuracy: 0.9294 - val_loss: 0.2197 - val_accuracy: 0.9104\n",
      "Epoch 4/20\n",
      "73/73 - 1s - loss: 0.1442 - accuracy: 0.9444 - val_loss: 0.1992 - val_accuracy: 0.9225\n",
      "Epoch 5/20\n",
      "73/73 - 1s - loss: 0.1152 - accuracy: 0.9568 - val_loss: 0.2366 - val_accuracy: 0.9117\n",
      "Epoch 6/20\n",
      "73/73 - 1s - loss: 0.0964 - accuracy: 0.9641 - val_loss: 0.2020 - val_accuracy: 0.9203\n",
      "Epoch 7/20\n",
      "73/73 - 1s - loss: 0.0811 - accuracy: 0.9717 - val_loss: 0.2291 - val_accuracy: 0.9138\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fea3ac4e8b0>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "callbacks = [tf.keras.callbacks.EarlyStopping(monitor = 'val_loss', patience = 3,restore_best_weights=True)]\n",
    "\n",
    "model.fit(\n",
    "    X_train, \n",
    "    y_train, \n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs = 20,\n",
    "    batch_size = 128,\n",
    "    callbacks=callbacks,\n",
    "    verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73/73 [==============================] - 0s 2ms/step - loss: 0.1992 - accuracy: 0.9225\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9224772453308105"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_val, y_val)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Positive, Negative and Neutral Tweets\n",
    "### Exercise 8:\n",
    "Now, we're going to use the three labels to create the model. But, first, encode the corresponding labels, split the data and transform it to NumPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tweets['airline_sentiment'] = tot_tweets['airline_sentiment'].map({'positive' : 2, 'neutral':1, 'negative': 0}).copy()\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(tweets['text'], tweets['airline_sentiment'], test_size = 0.2, random_state = 5)\n",
    "X_train = X_train.to_numpy()\n",
    "X_val = X_val.to_numpy()\n",
    "y_train = y_train.to_numpy()\n",
    "y_val = y_val.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 9:\n",
    "Repeat the previous procedure to create the vectorization layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size=10000\n",
    "seq_length = 50\n",
    "# Create a vectorization layer\n",
    "vectorize_layer = TextVectorization(\n",
    "    standardize = our_standardization,\n",
    "    max_tokens = vocab_size,\n",
    "    output_sequence_length = seq_length\n",
    "    )\n",
    "vectorize_layer.adapt(X_train)\n",
    "\n",
    "## Transform sequences of words to seq of integers and labels to tensor\n",
    "X_train = vectorize_layer(X_train)\n",
    "X_val = vectorize_layer(X_val)\n",
    "y_train = tf.convert_to_tensor(y_train)\n",
    "y_val = tf.convert_to_tensor(y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model `Embeddings`+`Conv1D`+`MaxPooling1D`+`Flatten`+`Dense`\n",
    "### Exercise 10:\n",
    "Modify the first model, i.e. `Embeddings`+`Conv1D`+`MaxPooling1D`+`Flatten`+`Dense`, to this problem (be aware of the expected output dimension and loss function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_5 (InputLayer)         [(None, 50)]              0         \n",
      "_________________________________________________________________\n",
      "embedding_4 (Embedding)      (None, 50, 16)            160000    \n",
      "_________________________________________________________________\n",
      "conv1d_6 (Conv1D)            (None, 43, 32)            4128      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 21, 32)            0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 672)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 3)                 2019      \n",
      "=================================================================\n",
      "Total params: 166,147\n",
      "Trainable params: 166,147\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "emb_size = 16\n",
    "num_filtes = 32\n",
    "ker_size = 8\n",
    "\n",
    "inputs = tf.keras.Input(shape = (seq_length, ))\n",
    "emb = layers.Embedding(input_dim=vocab_size, output_dim=emb_size)(inputs) \n",
    "x = layers.Conv1D(filters = num_filtes, kernel_size = ker_size, activation = 'relu')(emb)\n",
    "x = layers.MaxPooling1D(2)(x)\n",
    "x = layers.Flatten()(x)\n",
    "outputs = layers.Dense(3, activation='softmax')(x)\n",
    "model = tf.keras.Model(inputs, outputs)\n",
    "\n",
    "model.compile(optimizer=\"rmsprop\",\n",
    "              loss=\"sparse_categorical_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "73/73 - 1s - loss: 0.5491 - accuracy: 0.7900 - val_loss: 0.4356 - val_accuracy: 0.8064\n",
      "Epoch 2/20\n",
      "73/73 - 1s - loss: 0.3292 - accuracy: 0.8567 - val_loss: 0.2716 - val_accuracy: 0.8917\n",
      "Epoch 3/20\n",
      "73/73 - 1s - loss: 0.1941 - accuracy: 0.9252 - val_loss: 0.2061 - val_accuracy: 0.9186\n",
      "Epoch 4/20\n",
      "73/73 - 0s - loss: 0.1443 - accuracy: 0.9442 - val_loss: 0.1883 - val_accuracy: 0.9246\n",
      "Epoch 5/20\n",
      "73/73 - 0s - loss: 0.1160 - accuracy: 0.9565 - val_loss: 0.2072 - val_accuracy: 0.9177\n",
      "Epoch 6/20\n",
      "73/73 - 0s - loss: 0.0967 - accuracy: 0.9636 - val_loss: 0.1929 - val_accuracy: 0.9268\n",
      "Epoch 7/20\n",
      "73/73 - 1s - loss: 0.0817 - accuracy: 0.9709 - val_loss: 0.2172 - val_accuracy: 0.9164\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fea3c5975e0>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "earlystop = [tf.keras.callbacks.EarlyStopping(monitor = 'val_loss', patience = 3,restore_best_weights=True)]\n",
    "\n",
    "model.fit(\n",
    "    X_train, \n",
    "    y_train, \n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs = 20,\n",
    "    batch_size = 128,\n",
    "    callbacks=earlystop,\n",
    "    verbose = 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73/73 [==============================] - 0s 2ms/step - loss: 0.1883 - accuracy: 0.9246\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9246426820755005"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_val, y_val)[1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('adams')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "071737a5efb5187f1b8a7f5eacd9bb694a30cbbaa4393dd0a3bebb490d9d36dd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
