{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XyuoS1V_A74Q"
   },
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Humboldt-WI/adams/blob/master/demos/nlp/word-2-vec.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "skvR6YA3A74T"
   },
   "source": [
    "# Word Embeddings and Word-to-Vec (W2V)\n",
    "This demo notebook revisits the lecture on word embeddings and Google's word-to-vec algorithm. W2V, like backpropagation, is a very popular algorithm that enjoys much coverage in various blogs, youtube channels, etc. In case you appreciate some additional material to read-up on W2V, here here are some useful resources including,  \n",
    "- [the original W2V paper](https://proceedings.neurips.cc/paper/2013/file/9aa42b31882ec039965f3c4923ce901b-Paper.pdf)\n",
    "- the beautiful [\"Illustrated Word2vec\" by Jay Alammar](https://jalammar.github.io/illustrated-word2vec/)\n",
    "- the[W2V Tensorflow tutorial](https://www.tensorflow.org/tutorials/text/word2vec)\n",
    "\n",
    "Last but not least, our main textbook features excellent chapters on word embeddings, W2V, and related algorithms inlcuding GloVe and Fasttext. You can find those parts in [Section 14 of Dive into Deep Learning](http://d2l.ai/chapter_natural-language-processing-pretraining/index.html)\n",
    "\n",
    "Let's get started with our ADAMS demo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GdTgrnLmA75V",
    "tags": []
   },
   "source": [
    "## Training word-to-vec embeddings\n",
    "When it comes to embeddings, the most common use case is to **download pre-trained embeddings** and employ these for some downstream tasks (with or without fine-tuning). The Keras *embedding layer* supports that use case very well, as we will see in a future demo on sentiment analysis. Since this demo aims at deepening our understanding of W2V, we focus on a different use case and demonstrate the training of **customer word embeddings** using our IMDB data. \n",
    "\n",
    "You could argue that the IMDB forum exhibits a specific type of speech or jargon, and that this justifies training word embeddings for this specific corpus. In practice, using pre-trained embeddings will almost surely give better results than training embeddings from zero. However, without going into too much detail of the pros and cons of pre-training your own embeddings versus employing pre-trained embeddings, perhaps with some finetuning, the point of this section is simply to showcase how you could train from scratch if you want to. To that end, we will use a library called `Gensim`. \n",
    "\n",
    "`Gensim` is a popular library for text processing. Although maybe even more geared toward topic modeling, it offers, among others, implementations of several algorithms to learn word embeddings including *W2V*, *GloVe*, and *Fasttext*. We demonstrate training W2V embeddings using our cleaned IMDB movie review data set. Before moving on, make sure to have installed `Gensim`. \n",
    "\n",
    "**Credits and disclaimers**: many of the examples you are going to see in this section have been inspired by this very nice [Kaggle post](https://www.kaggle.com/code/pierremegret/gensim-word2vec-tutorial/notebook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2030,
     "status": "ok",
     "timestamp": 1649252392694,
     "user": {
      "displayName": "Georg B",
      "userId": "16990759077565939430"
     },
     "user_tz": -120
    },
    "id": "60p3_IyJXfnZ",
    "outputId": "5a96fca4-95b5-43c8-f4a8-a8f3925a50b7"
   },
   "outputs": [],
   "source": [
    "# Create a global variable to idicate whether the notebook is run in Colab\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "# Configure variables pointing to directories and stored files \n",
    "if IN_COLAB:\n",
    "    # Mount Google-Drive\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    DATA_DIR = '/content/drive/My Drive/'  # adjust to Google drive folder with the data if applicable\n",
    "else:\n",
    "    DATA_DIR = './' # adjust to the directory where data is stored on your machine (if running the notebook locally)\n",
    "\n",
    "sys.path.append(DATA_DIR)\n",
    "\n",
    "CLEAN_REVIEW = DATA_DIR + 'imdb_clean_full_v2.pkl'   # List with tokenized reviews after standard NLP preparation\n",
    "IMBD_EMBEDDINGS = DATA_DIR + 'w2v_imdb_full_d100_e500.model'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recap W2V\n",
    "Let's quickly revisit the principles of W2V. Please consult the paper of [Mikolov et al. (2013)](https://proceedings.neurips.cc/paper/2013/file/9aa42b31882ec039965f3c4923ce901b-Paper.pdf) for a detailed description.\n",
    "\n",
    "W2V establishes a word's meaning by the words that frequently appear close-by (distributional semantics). More specifically, the context of a word consists of the words that appear next to it within a pre-defined window (let's say 5 words).\n",
    "\n",
    " - the quality of *air* in mainland China has been decreasing since..\n",
    " - doctors claim the *air* you breath defines the overall wellbeing...\n",
    " - the currents of hot *air* have been bursting from underground\n",
    " - the mountain *air* was crystal clean and filled with ..\n",
    " - in case of *air* supply shortages, the submarine will..\n",
    "\n",
    "Taking the word *air* as our **target word**, the words around *air*, called context words, define the **meaning** of the word *air* in W2V.\n",
    "\n",
    "![w2vprocess](w2v.jpg)\n",
    "<br>\n",
    "inspired by https://www.youtube.com/watch?v=BD8wPsr_DAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4rmnnwM3UwrE",
    "tags": []
   },
   "source": [
    "### Loading the data\n",
    "We load the data frame with the original and cleaned reviews. The original version does not matter for this session. We will delete them to save memory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 581,
     "status": "ok",
     "timestamp": 1649251541851,
     "user": {
      "displayName": "Georg B",
      "userId": "16990759077565939430"
     },
     "user_tz": -120
    },
    "id": "eP6mlAonUwrF",
    "outputId": "e8d45569-5eb4-4c76-b88c-8bc2a1641f85"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 50000 entries, 0 to 49999\n",
      "Data columns (total 3 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   review        50000 non-null  object\n",
      " 1   sentiment     50000 non-null  object\n",
      " 2   review_clean  50000 non-null  object\n",
      "dtypes: object(3)\n",
      "memory usage: 1.1+ MB\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "with open(CLEAN_REVIEW,'rb') as path_name:\n",
    "    df = pickle.load(path_name)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 231,
     "status": "ok",
     "timestamp": 1649251543101,
     "user": {
      "displayName": "Georg B",
      "userId": "16990759077565939430"
     },
     "user_tz": -120
    },
    "id": "r0FiXQGCUwrL",
    "outputId": "0be4e472-f42a-4999-cc9c-b41648d98a6f",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>positive</td>\n",
       "      <td>one reviewer mention watch oz episode hooked r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>positive</td>\n",
       "      <td>wonderful little production film technique una...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>positive</td>\n",
       "      <td>thought wonderful way spend time hot summer we...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>negative</td>\n",
       "      <td>basically family little boy jake think zombie ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>positive</td>\n",
       "      <td>petter love time money visually stun film watc...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  sentiment                                       review_clean\n",
       "0  positive  one reviewer mention watch oz episode hooked r...\n",
       "1  positive  wonderful little production film technique una...\n",
       "2  positive  thought wonderful way spend time hot summer we...\n",
       "3  negative  basically family little boy jake think zombie ...\n",
       "4  positive  petter love time money visually stun film watc..."
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop(labels=\"review\", axis=1, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "liLqBv7CA75V"
   },
   "source": [
    "### The Gensim W2V model\n",
    "Training word embeddings using `Gensim` is very easy and just a matter of calling a function. Well, the reason it takes so little code is that we have already cleaned our data and have it available as an array of texts; that is a format that `Gensim`supports. However, note that, depending on your data, the code may take quite a while to run. Again, word embeddings trained on the full 50K data set for 500 epochs are available in our course folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need a bit of infrstructure to run the algorithm\n",
    "from gensim import utils\n",
    "\n",
    "class CleanReviews:\n",
    "    \"\"\"An iterator that yields sentences (lists of str).\"\"\"\n",
    "    \n",
    "    def __init__(self, reviews):\n",
    "        self.reviews = reviews\n",
    "        \n",
    "    def __iter__(self):\n",
    "        for line in self.reviews:\n",
    "            yield utils.simple_preprocess(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "HN4TnV8zA75W"
   },
   "outputs": [],
   "source": [
    "# CAUTION: Running the code might take a while\n",
    "from gensim.models import Word2Vec    \n",
    "\n",
    "emb_dim = 10  # embedding dimension, we use 10 for a quick demo of the code\n",
    "reviews = CleanReviews(df.review_clean)\n",
    "# Train a Word2Vec model\n",
    "model = Word2Vec(sentences=reviews, \n",
    "                 min_count=5,  # min_count means the word frequency threshold, if =2 and word is used only once - it's not included\n",
    "                 window=5,     # the size of context window\n",
    "                 epochs=5,     # epochs is set to 5 to decrease runtim, would be much larger in practice\n",
    "                 vector_size=emb_dim,  # size of embedding\n",
    "                 workers=2)    # for parallel computing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure to check out the docstring of the `Word2Vec` function to discover how word vectors are trained by default. Importantly, the argument `sg` let's you chose between *skip-gram* and *cbow*. Other concepts we discussed in the lecture include accelerating computations using *hierarchical softmax* and *negative sampling*. Gensim features these through its arguments `hs` and `negative`, respectively. Obviously, tons of other functionality is available, so make sure to study the [documentation](https://radimrehurek.com/gensim/models/word2vec.html?highlight=word2vec) if you plan to use the Gensim library for serious projects. Also, just to remind you, the [Kaggle post](https://www.kaggle.com/code/pierremegret/gensim-word2vec-tutorial/notebook), which inspired this notebook, has a slightly more elaborate demo of how to set up training and, specifically, how you can break down the individual steps of W2V training into smaller pieces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The trained word vectors are accessible through the field `wv` of the model class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.9756973  1.1006564  2.1407259 -2.7641182  4.1969585 -3.8379216\n",
      " -1.5564842  1.7839793 -1.1708127  1.1585494]\n"
     ]
    }
   ],
   "source": [
    "# what is the word vector of the words good and bad?\n",
    "print(model.wv['good'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.03799623  1.7121277   3.5084138  -3.0625455   2.6999998  -5.3605113\n",
      " -1.6722724   3.313875   -0.6442533  -1.1794131 ]\n"
     ]
    }
   ],
   "source": [
    "print(model.wv['bad'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30201"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.wv.key_to_index)  # how many word vectors have been trained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We continue with playing with word vectors shortly but let us first discuss input and output handling with Gensim."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dKjhdPPAA75X"
   },
   "source": [
    "### Input / output handling\n",
    "Gensim supports saving and loading of trained embeddings in different versions. This makes a lot of sense since training can take a long time. For example, you could train for a couple of epochs, then store your results on disk, and then continue training. Here is how we can store our trained word vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "Bo93egFsA75Y"
   },
   "outputs": [],
   "source": [
    "# Save trained word vectors to disk\n",
    "file=\"w2v_tmp.model\"\n",
    "save_as_bin = False\n",
    "model.wv.save_word2vec_format(file, binary=save_as_bin)  # set binary to True to save disk space; false facilitates inspecting the embeddings in a text editor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dKjhdPPAA75X"
   },
   "source": [
    "For Adams, you can obtain word vectors trained on the IMDB corpus for 500 epochs from our [GitHub repository](https://github.com/Humboldt-WI/adams/tree/master/demos/nlp). These vectors are far from comparable to real pre-trained W2V embeddings. On the other hand, their training took a couple of hours so the vectors should carry a bit more information compared to just running the above training code with a small embedding dimension of ten and training for only five epochs. Let's showcase how we can save and load word vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "eD7dCexjA75Z"
   },
   "outputs": [],
   "source": [
    "# Load model from disk\n",
    "from gensim.models import KeyedVectors\n",
    "w2v = KeyedVectors.load_word2vec_format(IMBD_EMBEDDINGS, binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that you can also access the `KeyedVectors`, which we load with the previous statement, directly from a trained model object via the field `wv`. Thus, if you would like to run the following demos with the word vectors you trained yourself, simply run the following command. One would expect that the demos give nicer results with the pre-trained embeddings from your repo but you are welcoem to try this our yourself. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# w2v = model.wv  # continue with the W2V embeddings trained above "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZYt70SnBA75Z",
    "tags": []
   },
   "source": [
    "### Playing with embeddings\n",
    "Again, the embeddings loaded above are far from solid but should give us some somewhat meaningful results in algebraic comparisons. Let's see whether this works out. \n",
    "\n",
    "#### Which word is most similar to another word?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lewhU1G7A75a",
    "outputId": "ab90e32d-b95e-4f35-ef74-3b7ff671729b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('least', 0.9374186396598816),\n",
       " ('probably', 0.9304987192153931),\n",
       " ('still', 0.9181867837905884),\n",
       " ('ever', 0.9113300442695618),\n",
       " ('definitely', 0.9088698625564575),\n",
       " ('even', 0.9075060486793518),\n",
       " ('honestly', 0.9061371088027954),\n",
       " ('lately', 0.8953308463096619),\n",
       " ('actually', 0.8867319226264954),\n",
       " ('watch', 0.8728312253952026)]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.most_similar(positive=['movie'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7v4DJ1egA75b"
   },
   "source": [
    "#### How similar are two words?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P22xPLE5A75c",
    "outputId": "ed2f54db-b147-4e8d-9036-aee3b1c3e5c2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.86766267"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.similarity('good', 'great')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z5pxaVl8A75c",
    "outputId": "3775a2eb-3a1b-47ad-b06b-091fe28615a1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How similar is Tarantino to Spielberg: 0.8217860460281372\n",
      "How similar is Lucas to Spielberg: 0.7440744042396545\n",
      "How similar is Paltrow to Bullock: 0.7634718418121338\n",
      "How similar is Paltrow to Alba: 0.8613905310630798\n",
      "How similar is Cruise to Depp: 0.5666610598564148\n",
      "How similar is Cruise to Willis: 0.6183159947395325\n"
     ]
    }
   ],
   "source": [
    "print('How similar is Tarantino to Spielberg: {}'.format(w2v.similarity('tarantino', 'spielberg')))\n",
    "print('How similar is Lucas to Spielberg: {}'.format(w2v.similarity('lucas', 'spielberg')))\n",
    "\n",
    "print('How similar is Paltrow to Bullock: {}'.format(w2v.similarity('paltrow', 'bullock')))\n",
    "print('How similar is Paltrow to Alba: {}'.format(w2v.similarity('paltrow', 'alba')))\n",
    "\n",
    "print('How similar is Cruise to Depp: {}'.format(w2v.similarity('cruise', 'depp')))\n",
    "print('How similar is Cruise to Willis: {}'.format(w2v.similarity('cruise', 'willis')))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nxlk22MJA75d"
   },
   "source": [
    "#### Which word does not fit in?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 93
    },
    "id": "JPYGN35DA75d",
    "outputId": "b974bffc-2b12-4523-fba7-2567de668023"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weak\n",
      "good\n"
     ]
    }
   ],
   "source": [
    "print(w2v.doesnt_match(['cool', 'great', 'lovely', 'weak']))\n",
    "print(w2v.doesnt_match(['movie', 'film', 'good']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "htZvGR4YA75e"
   },
   "source": [
    "#### A is to B as C is to ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h-DTdgxNA75e",
    "outputId": "6621430d-aafd-42a6-c198-51eabc845099"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('deserves', 0.9519431591033936),\n",
       " ('deserve', 0.8851733803749084),\n",
       " ('thanks', 0.8781426548957825),\n",
       " ('surpass', 0.867881178855896),\n",
       " ('qualify', 0.8677225708961487)]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.most_similar(positive=['spielberg', 'woman'], negative=['man'], topn=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QeiOWgJkA75h"
   },
   "source": [
    "### Phrase detection\n",
    "W2V trains one embedding per word. The model is agnostic of common phrases such as 'New York'. It would train one embedding for new and another for york, provided both words are part of the vocabulary. You can get better embeddings by adding common phrases to the vocabulary. W2V will then train individual embeddings for these phrases. Gensims also comes with a phrase detection models, which allows you to handle bigrams, trigrams and the like. We will not retrain our W2V model but sketch how you can use Gensim to get these common phrases. You could then consider to add (some of) them to your vocab and enhance the model.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "JgFV1l0SA75h"
   },
   "outputs": [],
   "source": [
    "from gensim.models.phrases import Phrases, ENGLISH_CONNECTOR_WORDS\n",
    "# Train a bigram model\n",
    "bigram_model = Phrases(sentences=reviews,min_count=10 , threshold=1, connector_words=ENGLISH_CONNECTOR_WORDS) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EdmEVqvdA75h"
   },
   "source": [
    "After training, we can take text and put it through the bigram model. The model will then alter the text so as to introduce bigrams. Here is an example,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AzZUbniKA75h",
    "outputId": "0edab39a-866b-4f3a-fcd0-5968295d2bb4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'like', 'this', 'movie']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to process text and replace phrases, we use our phrase detector as follows\n",
    "bigram_model['I', 'like', 'this', 'movie']  # no phrases to be detected here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AzZUbniKA75h",
    "outputId": "0edab39a-866b-4f3a-fcd0-5968295d2bb4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sex', 'and', 'the', 'city', 'is', 'all', 'about', 'new_york']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_model['sex', 'and', 'the', 'city', 'is', 'all', 'about', 'new', 'york']  # but we would expect city names to be detected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wGbE6LWEA75j"
   },
   "source": [
    "We can also make use of our counter class to examine the most common bigrams in the corpus, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "JefH9K6TA75j"
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "bigram_counter = collections.Counter()\n",
    "for key in bigram_model.vocab.keys():\n",
    "    if key.find('_')>-1: # the decode is needed because Gensims stores keys as bytes\n",
    "        bigram_counter[key] += bigram_model.vocab[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "72LnWyn2A75k",
    "outputId": "8d0cc7c8-6c26-4ef3-a7a1-6707798e8631"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('look_like', 3715),\n",
       " ('watch_movie', 3121),\n",
       " ('ever_see', 2973),\n",
       " ('see_movie', 2752),\n",
       " ('bad_movie', 2727),\n",
       " ('make_movie', 2392),\n",
       " ('year_old', 2389),\n",
       " ('film_make', 2369),\n",
       " ('special_effect', 2308),\n",
       " ('movie_make', 2134),\n",
       " ('one_best', 2030),\n",
       " ('even_though', 1999),\n",
       " ('movie_ever', 1987),\n",
       " ('movie_like', 1921),\n",
       " ('low_budget', 1892),\n",
       " ('make_film', 1882),\n",
       " ('see_film', 1859),\n",
       " ('main_character', 1838),\n",
       " ('waste_time', 1793),\n",
       " ('watch_film', 1664),\n",
       " ('good_movie', 1634),\n",
       " ('horror_movie', 1611),\n",
       " ('much_well', 1532),\n",
       " ('want_see', 1494),\n",
       " ('seem_like', 1473)]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_counter.most_common(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZKzSOo6NA75k"
   },
   "source": [
    "The above bigrams might be frequent. However, you would not consider training individual embeddings for phrases such as *look_like* or *waste_time*. This shows how proper phrase detection in the scope of W2V is nontrivial and would require more work before we can hope to get good results.     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting word vectors\n",
    "It is fairly easy to create a visualization of the trained word vectors. You can find an example of how to do this in the [Kaggle kernel](https://www.kaggle.com/pierremegret/gensim-word2vec-tutorial) mentioned above. Needless to say, many alternative demos are available online; here is just [one example](https://www.kaggle.com/jeffd23/visualizing-word-vectors-with-t-sne). However, to get meaningful results we would need to prepare the data more carefully by, for example, removing too frequent words and too infrequent words. We would also finetune the training, and, overall, invest a lot more work to craft our word embeddings. In practice, we would typically not train our own embeddings from scratch. Instead, we would download pre-trained embeddings, which are available in many flavors (multiple languages, trained on different corpora with different jargon, etc.), and use these in our NLP application. We could also finetune the pre-trained embeddings using our own text data. We will showcase a corresponding approach in a later notebook on sentiment analysis. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Manual Word2Vec using Keras\n",
    "\n",
    "In the following, we will re-implement W2V in Keras. Remember that W2V proposes two models for learning word vectors, continuous-bag-of-words (CBOW) and Skip-Gram. In a nutshell, CBOW predicts a central target word from surrounding context words, while Skip-Gram takes the opposite approach. Given a <font color='red'>target word</font>, predict <font color='green'>context words</font> with high chance to appear next to the target word in a corpus. Considering one of the above example sentences and a widow size of 2, we can highlight target and context words as follows:<br><br>\n",
    "[doctors <font color='green'>claim the</font><font color='red'> air </font><font color='green'>you breath</font> defines]. \n",
    "<br><br>Using a question mark to indicate the target variable of the model, we obtain:\n",
    "\n",
    "[doctors *? ?* **air** *? ?* breath] in Skip-Gram versus [doctors *claim the* **?** *you breath* defines] in CBOW.\n",
    "\n",
    "\n",
    "In this section, we focus on Skip-Gram, which seems to be the preferred approach in practice. The code is based on a nice tutorial by [Dipanjan Sarkar](https://towardsdatascience.com/understanding-feature-engineering-part-4-deep-learning-methods-for-text-data-96c44370bbfa), in which you can also find a Keras implementation of CBOW; if interested. However, as nice as the post is, the code is not compatible with the recent version of Keras, which is the one you probably use (i.e., Keras 2). So we will take care of that issue in our implementation.  \n",
    "\n",
    "Before moving on, let's remember the architecture of the skip-gram W2V model.\n",
    "\n",
    "![sg](https://upload.wikimedia.org/wikipedia/commons/9/95/Skip-gram.png)\n",
    "<br>\n",
    "Source: https://upload.wikimedia.org/wikipedia/commons/9/95/Skip-gram.png\n",
    "\n",
    "Given a sentence - better to say sequence of text - we take a target word and predict a set of context words, that is, words, which appear in a certain <font color=\"green\">**context window**</font>  $[w_{-i},\\ldots, w, \\ldots, w_{+i}]$, where $i$ is the *window size* and the number of context words to consider is window size $\\times 2$. \n",
    "\n",
    "An important caveat with the above picture is that a corresponding model would not scale. Remember that the output layer involves a high-dimensional softmax which is too costly to compute for any reasonably sized corpus. Among the two options around this problem, *hierarchical softmax* and *negative sampling*, we will make use of the latter. So given a target word, our prediction task will be to classify whether another word is an actual context word for that target word, or a random word sampled from the corpus according to some probability distribution. This is a binary classification tasks. Thus, the output of our neural network is must cheaper to compute. Instead of a high-dimensional softmax we only need a simple logistic classifier. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the vocabulary\n",
    "Let's start with building our vocabulary. It is common practice to not train to train every word but words that occur reasonably frequent. For rare words, training a good embedding is difficult. Remember how this issue motivated subword embeddings like Fasttext. In our example, we simply use the most frequent words from the review corpus and try to compute embeddings for these words. This is the point where our word_counter (see above) comes in handy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use our cleaned IMDB data set for the demo\n",
    "with open(CLEAN_REVIEW,'rb') as path_name:\n",
    "    df_imdb = pickle.load(path_name)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['movie', 'film', 'one', 'make', 'like', 'see', 'get', 'well', 'time', 'good']"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This code is copied from the NLP foundations notebook. \n",
    "word_counter = collections.Counter()\n",
    "for r in df_imdb[\"review_clean\"]:\n",
    "    for w in r.split():  # this is like tokenizing using the white space\n",
    "        word_counter.update({w: 1})\n",
    "\n",
    "# Extract the n most common words from the corpus\n",
    "vocab_size = 1000\n",
    "vocab = word_counter.most_common(vocab_size)\n",
    "vocab = [x[0] for x in vocab]\n",
    "vocab[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next task is to build dictionary. For Keras, we need to encode words as integers, which Keras will then interpret as indices into a one-hot vector of the size of the vocabulary. We build two dictionaries. One to map words to their code (i.e., unique integer) and one to revert the mapping and decode words. \n",
    "\n",
    "In the below code, we implicitly exploit the fact that our vocabulary is ordered by frequency. The most frequent word receives the index 1, the second-most frequent word the index two, and so forth. That will prove useful later when calculating sampling weights for the negative sampling. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = range(1, vocab_size)\n",
    "word2id = dict(zip(vocab, idx))\n",
    "id2word = dict(zip(idx, vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 1000\n",
      "Vocabulary Sample: [('movie', 1), ('film', 2), ('one', 3), ('make', 4), ('like', 5), ('see', 6), ('get', 7), ('well', 8), ('time', 9), ('good', 10)]\n",
      "[('mood', 990), ('regard', 991), ('jane', 992), ('garbage', 993), ('reference', 994), ('barely', 995), ('haunt', 996), ('super', 997), ('humour', 998), ('impressive', 999)]\n"
     ]
    }
   ],
   "source": [
    "print('Vocabulary size: {}'.format(vocab_size))\n",
    "print('Vocabulary Sample:', list(word2id.items())[:10])\n",
    "print(list(word2id.items())[-10:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may have noted that we have so far left out the index 0. This index is commonly reserved for unknown words, which we map to a special token. Rmember that our vocabulary is not very large when compared to the number of words that exists in a language (e.g., ~300K in English). So when processing texts, we will run into a lot of unknown words. We deal with these words by mapping them to the token `UNK`. This way, we learn one embedding for all unknown words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2id[\"UNK\"] = 0\n",
    "id2word[0] = \"UNK\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to map unknown words to index 'unknown'\n",
    "def encode_review(review, dictionary):\n",
    "    output = []\n",
    "    for word in review:\n",
    "        if word not in dictionary.keys():\n",
    "            output.append(dictionary[\"UNK\"])\n",
    "        else:\n",
    "            output.append(dictionary[word])\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to turn our reviews into integer numbers, which is the format that Keras expects, while accounting for unknown words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "#* Build the corpus for W2V by encoding the reviews\n",
    "coded_review = []\n",
    "for r in df_imdb[\"review_clean\"]:\n",
    "    coded_review.append(encode_review(r.split(), word2id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encourage positive comment film look forward watch film bad mistake see film truly one bad awful almost every way edit pace storyline soundtrack song lame country tune played less four time film look cheap nasty boring extreme rarely happy see end credit film thing prevents give score harvey keitel far best performance least seem make bit effort one keitel obsessive\n",
      "[0, 928, 306, 2, 22, 687, 11, 2, 14, 840, 6, 2, 276, 3, 14, 280, 123, 85, 35, 432, 418, 614, 583, 257, 677, 442, 0, 162, 251, 519, 9, 2, 22, 556, 0, 267, 0, 0, 413, 6, 25, 379, 2, 37, 0, 32, 385, 0, 0, 134, 49, 65, 129, 40, 4, 107, 453, 3, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# Some testing\n",
    "id_demo_review = 8  # one random review\n",
    "demo_review = df_imdb[\"review_clean\"][id_demo_review]\n",
    "print(demo_review)  # plain text after cleaning\n",
    "\n",
    "# One-hot-coding representation in which integer numbers represent the\n",
    "# index of the single non-zero element in a one-hot-vector of dimensionality \n",
    "# vocab_size\n",
    "print(coded_review[id_demo_review])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looks good\n"
     ]
    }
   ],
   "source": [
    "if len(coded_review[id_demo_review]) == len(demo_review.split()):\n",
    "    print(\"Looks good\")\n",
    "else:\n",
    "    print(\"that can't be right\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Generate training data\n",
    "Th training data for our skip-gram model consists of tuples (target, context) with corresponding label (0/1), indicating whether the second word really appeared in the context of the target word our not. Fortunately, Keras has a ready-made function that we can use for that purpose. Specifically, the function `skipgrams` takes a sentence as input and outputs:\n",
    "\n",
    "1. target words in combination with a context word\n",
    "2. a label if the context word is from the actual context or randomly sampled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import skipgrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first illustrate the function *skipgrams()* for a single short sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read book forget movie\n",
      "[234, 144, 626, 1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('read', 234), ('book', 144), ('forget', 626), ('movie', 1)]"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Produce a list of review lengths\n",
    "r_lengths  = [len(r) for r in coded_review]\n",
    "# Indices of reviews ordered by their length in ascending order\n",
    "ix = np.argsort(r_lengths)\n",
    "\n",
    "# The second shortest review in the data set is a good example (you can use any review)\n",
    "print(df_imdb[\"review_clean\"][ix[1]])\n",
    "\n",
    "# Encoded version\n",
    "print(coded_review[ix[1]])\n",
    "\n",
    "# Just for fun, we can of course also decode the one-hot-encoding    \n",
    "[(id2word[c],c) for c in coded_review[ix[1]]]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_sentence = coded_review[ix[1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that a window size of `i` translates to $[w_{-i},\\ldots, w, \\ldots, w_{+i}]$, so the number of context words to consider is window size $\\times 2$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(read (234), book (144))\t -> 1\n",
      "(read (234), laugh (120))\t -> 0\n",
      "(forget (626), read (234))\t -> 1\n",
      "(read (234), manages (855))\t -> 0\n",
      "(book (144), attention (535))\t -> 0\n",
      "(movie (1), disappointed (540))\t -> 0\n",
      "(book (144), lose (227))\t -> 0\n",
      "(forget (626), movie (1))\t -> 1\n",
      "(book (144), forget (626))\t -> 1\n",
      "(read (234), forget (626))\t -> 1\n"
     ]
    }
   ],
   "source": [
    "pairs, labels = skipgrams(example_sentence, vocabulary_size=vocab_size, window_size=window_size)\n",
    "for i in range(10):\n",
    "    print(\"({:s} ({:d}), {:s} ({:d}))\\t -> {:d}\".format( id2word[pairs[i][0]], \n",
    "                                                        pairs[i][0], \n",
    "                                                        id2word[pairs[i][1]],\n",
    "                                                        pairs[i][1], labels[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above demo, negative samples not appearing in the context window of the target words were picked at random. According to empirical evidence, the probability of a word to be sampled as negative example should be related to its frequency. Otherwise, we might end up with focussing too much on frequent words. Keras provides a utility function, *make_sampling_table*, to calculate sampling weights for each word in the corpus. Details are available in the [Keras documentation](https://keras.io/preprocessing/sequence/). The `sampling_table` is a list of sampling probabilities, one for each word. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00315225, 0.00315225, 0.00547597, 0.00741556, 0.00912817,\n",
       "       0.01068435, 0.01212381, 0.01347162, 0.01474487, 0.0159558 ])"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import make_sampling_table\n",
    "samp_tab = make_sampling_table(vocab_size)\n",
    "samp_tab[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the increasing magnitude of the sampling weights. Sampling words from the corpus using this sampling distribution requires an that the words in the corpus are ordered by frequency. Remember the idea is that when sampling negative examples we do not want to focus too much on the frequent words; in our case words like 'movie', and 'film', and 'like'. Therefore, we raise the chance of less frequent words to be sampled as negative examples. \n",
    "\n",
    "When building our corpus above, we used the *most_common()* function. Therefore, the words in our corpus are ordered in decreasing order by their frequency. We will make use of our sampling table to govern the sampling of negative examples when generating the training set for our W2V model.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 50000 skip-grams in 7.139723777770996 sec.\n"
     ]
    }
   ],
   "source": [
    "# CAUTION: yet another operation that is not cheap when using all data\n",
    "import time\n",
    "start = time.time()\n",
    "skip_grams = [skipgrams(r, vocabulary_size=vocab_size, \n",
    "                        window_size=window_size, sampling_table=samp_tab) \n",
    "             for r in coded_review]\n",
    "end = time.time()\n",
    "print('Generated {} skip-grams in {} sec.'.format(len(skip_grams), end-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Building the neural network\n",
    "We are ready to design our NN architecture using Keras. We feed the network with pairs of target word and actual/fake context word. Each word is put through an embedding layer. Remember that W2V trains two embeddings per word, one when the word is the target word and one when the word appears in the context of some other target word. So using two embedding layers is important.\n",
    "\n",
    "Having obtained word embeddings for the target and context word, we pass these embeddings to a merge layer in which we compute the dot product of these two vectors. We can think of the dot products as an unnormalized cosine similarity between the two embedding vectors. Put differently, we obtain a similarity score. We want that score to be large when the inputted 'context' word actually appeared in the context of the target word, and small otherwise. Hence, we forward the similarity score to a dense sigmoid layer, which computes a probability of the 'context' word being an actual context word. We then compare this probability, the output of our neural network, to the actual label, which we obtained above from *skipgram()*. Enter back-propagation. \n",
    "\n",
    "So far so good, but there is one issue. Our network is a little more advanced than those we have built so far. There were also some changes when moving to Keras 2., which hit us in this example. Long story short, we cannot use the nice and simple sequential API anymore and will have to use the functional API instead. For this reason, the code will look a little different from what you are used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Model\n",
    "from keras.preprocessing import sequence\n",
    "from keras.layers import Embedding, Input, Reshape, Dot, Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding dimension\n",
    "emd_dim = 25  # relatively small but we do not use much data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up embedding layers for the target and the context word:\n",
    "embedding_target = Embedding(vocab_size, emd_dim, input_length=1, name='embedding_target')\n",
    "embedding_context = Embedding(vocab_size, emd_dim, input_length=1, name='embedding_context')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model architecture using the functional API\n",
    "\n",
    "# Take a single target word\n",
    "input_target = Input((1,))\n",
    "target = embedding_target(input_target)\n",
    "target = Reshape((emd_dim, 1))(target)\n",
    "\n",
    "# Take another word either from the context or a random word from vocabulary\n",
    "input_context = Input((1,))\n",
    "context = embedding_context(input_context)\n",
    "context = Reshape((emd_dim, 1))(context)\n",
    "\n",
    "# Calculate the dot product as an unnormalized cosine distance\n",
    "dot_product = Dot(axes=1, normalize=False)([target, context])\n",
    "dot_product = Reshape((1,))(dot_product)\n",
    "\n",
    "# Predict if the words are in the same context -> Binary yes/no\n",
    "output = Activation(activation='sigmoid')(dot_product)\n",
    "\n",
    "# Compile the model\n",
    "model = Model(inputs=[input_target, input_context], outputs=output)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See how the model is not much of a neural network? The only trainable parameters are the embeddings, which are then dot-multiplied. We thus have two hidden layers side-by-side rather than one after the other and no non-linear activation of the hidden layers! This is very similar to matrix factorization and you can use the same architecture to build a collaborative filter on users (one embedding matrix) and items (one embedding matrix); just in case you are into recommender engines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_target (Embedding)    (None, 1, 25)        25000       input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_context (Embedding)   (None, 1, 25)        25000       input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "reshape (Reshape)               (None, 25, 1)        0           embedding_target[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 25, 1)        0           embedding_context[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "dot (Dot)                       (None, 1, 1)         0           reshape[0][0]                    \n",
      "                                                                 reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_2 (Reshape)             (None, 1)            0           dot[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 1)            0           reshape_2[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 50,000\n",
      "Trainable params: 50,000\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a maybe more intuitive visualization of the model thanks to [Dipanjan Sarkar.](https://towardsdatascience.com/understanding-feature-engineering-part-4-deep-learning-methods-for-text-data-96c44370bbfa)\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/1123/1*4Uil1zWWF5-jlt-FnRJgAQ.png\">\n",
    "<br>\n",
    "Image source: \n",
    "https://miro.medium.com/max/1123/1*4Uil1zWWF5-jlt-FnRJgAQ.png2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop\n",
    "We train our model review by review, updating the model after every review (i.e., batch). Implementing this approach is not possible when using the standard Keras training loop. Therefore, we use the function *train_on_batch*, which gives us more control over the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Epoch 0\n",
      "----------------------------------------\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [156]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m couples:\n\u001b[0;32m     18\u001b[0m         X \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(couples, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mint32\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 19\u001b[0m         loss \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_on_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mX\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mX\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m         losses\u001b[38;5;241m.\u001b[39mappend(loss)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAverage loss over last 1000 batches: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp\u001b[38;5;241m.\u001b[39mmean(losses[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1000\u001b[39m:])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\deeplearn\\lib\\site-packages\\keras\\engine\\training.py:1852\u001b[0m, in \u001b[0;36mModel.train_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight, reset_metrics, return_dict)\u001b[0m\n\u001b[0;32m   1849\u001b[0m _disallow_inside_tf_function(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_on_batch\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m   1850\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistribute_strategy\u001b[38;5;241m.\u001b[39mscope(), \\\n\u001b[0;32m   1851\u001b[0m      training_utils\u001b[38;5;241m.\u001b[39mRespectCompiledTrainableState(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m-> 1852\u001b[0m   iterator \u001b[38;5;241m=\u001b[39m \u001b[43mdata_adapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msingle_batch_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistribute_strategy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1853\u001b[0m \u001b[43m                                                \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1854\u001b[0m \u001b[43m                                                \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1855\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_function \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmake_train_function()\n\u001b[0;32m   1856\u001b[0m   logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_function(iterator)\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\deeplearn\\lib\\site-packages\\keras\\engine\\data_adapter.py:1633\u001b[0m, in \u001b[0;36msingle_batch_iterator\u001b[1;34m(strategy, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[0;32m   1630\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1631\u001b[0m   data \u001b[38;5;241m=\u001b[39m (x, y, sample_weight)\n\u001b[1;32m-> 1633\u001b[0m \u001b[43m_check_data_cardinality\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1634\u001b[0m dataset \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataset\u001b[38;5;241m.\u001b[39mfrom_tensors(data)\n\u001b[0;32m   1635\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m class_weight:\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\deeplearn\\lib\\site-packages\\keras\\engine\\data_adapter.py:1642\u001b[0m, in \u001b[0;36m_check_data_cardinality\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m   1641\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_data_cardinality\u001b[39m(data):\n\u001b[1;32m-> 1642\u001b[0m   num_samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mset\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1643\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(num_samples) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   1644\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mData cardinality is ambiguous:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\deeplearn\\lib\\site-packages\\keras\\engine\\data_adapter.py:1642\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   1641\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_data_cardinality\u001b[39m(data):\n\u001b[1;32m-> 1642\u001b[0m   num_samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(\u001b[38;5;28mint\u001b[39m(\u001b[43mi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mnest\u001b[38;5;241m.\u001b[39mflatten(data))\n\u001b[0;32m   1643\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(num_samples) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   1644\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mData cardinality is ambiguous:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'int' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "# Number of epochs\n",
    "nb_epoch = 5\n",
    "\n",
    "for e in range(nb_epoch):\n",
    "        print('-'*40)\n",
    "        if e>0:\n",
    "            print('Epoch {} elapsed {:.2f} min.'.format(e, (end-start)/60))\n",
    "        else:\n",
    "            print('Epoch {}'.format(e))\n",
    "        print('-'*40)\n",
    "        start = time.time()\n",
    "\n",
    "        samples_seen = 0\n",
    "        losses = []\n",
    "        \n",
    "        for couples, labels in skip_grams:\n",
    "            if couples:\n",
    "                X = np.array(couples, dtype=\"int32\")\n",
    "                loss = model.train_on_batch([X[:,0],X[:,1]], labels)\n",
    "                losses.append(loss)\n",
    "        print(f'Average loss over last 1000 batches: {np.mean(losses[-1000:])}')\n",
    "        end = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Extracting the weights\n",
    "We can extract the word embeddings from the corresponding layer of our model. Converting the embeddings to a data frame facilitates a quick look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embeddings = model.get_layer(name=\"embedding_target\").get_weights()[0]\n",
    "print(word_embeddings.shape)\n",
    "w2v_df = pd.DataFrame(word_embeddings, index=id2word.values())\n",
    "w2v_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can try to reproduce some of the functionality demonstrated above for the Gensim implementation. Of course, we don't go all the way. Still, doing a little similarity calculation is not too difficult. We use some scikit-learn functionality to create a matrix of pairwise distances between words. We can then query the most similar words to some seed-words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "\n",
    "distance_matrix = euclidean_distances(word_embeddings)\n",
    "print(distance_matrix.shape)\n",
    "\n",
    "# Note that this code will not work if you trained on a small corpus\n",
    "# To make it work, you have to ensure that your search words are part of the vocabulary.\n",
    "similar_words = {search_term: [id2word[idx] for idx in distance_matrix[word2id[search_term]-1].argsort()[1:6]+1] \n",
    "                   for search_term in ['tarantino', 'cruise', 'willis', 'lawrence', 'bullock']}\n",
    "\n",
    "similar_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, we might want to continue training our embeddings.   "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "P.II.2_nlp_foundations.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
